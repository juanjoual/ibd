{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb733b2",
   "metadata": {},
   "source": [
    "# AnÃ¡lisis de prÃ©stamos de libros en bibliotecas de Madrid con Hadoop\n",
    "\n",
    "Â¿CuÃ¡les son los libros mÃ¡s prestados en las bibliotecas pÃºblicas de Madrid? En este notebook responderemos a esa pregunta procesando un dataset real de millones de registros de prÃ©stamos bibliotecarios. A lo largo del ejercicio, descargaremos los datos, escribiremos un job MapReduce en Python para contar prÃ©stamos por tÃ­tulo de libro, lo ejecutaremos tanto en local como en un clÃºster Hadoop, y finalmente visualizaremos los resultados.\n",
    "\n",
    "**Dataset:** PrÃ©stamos de las bibliotecas pÃºblicas de Madrid (2025-26) en formato CSV: [https://datos.madrid.es/dataset/212700-0-bibliotecas-prestamos-historico/information](https://datos.madrid.es/dataset/212700-0-bibliotecas-prestamos-historico/information)\n",
    "\n",
    "## Contenido\n",
    "1. **Descarga del dataset**\n",
    "2. **MapReduce local: Mapper y Reducer en Python**\n",
    "3. **Crear directorios y subir ficheros a HDFS**\n",
    "4. **Inspeccionar ficheros en HDFS**\n",
    "5. **Cambiar la replicaciÃ³n de un fichero**\n",
    "6. **MapReduce con Hadoop Streaming**\n",
    "7. **Descargar resultados y visualizar con Python**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f20ec5b",
   "metadata": {},
   "source": [
    "## 1. Descarga del dataset\n",
    "\n",
    "Descargamos el fichero CSV de prÃ©stamos bibliotecarios de Madrid. Este dataset contiene informaciÃ³n sobre cada prÃ©stamo: cÃ³digo de barras, biblioteca, tipo de material, tÃ­tulo, autor, fecha, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c010b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "!wget -O data/prestamos_bibliotecas_madrid_2025_26.csv \"http://192.168.70.194/prestamos_bibliotecas_madrid_2025_26.csv\"\n",
    "!wc -l data/prestamos_bibliotecas_madrid_2025_26.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 10 data/prestamos_bibliotecas_madrid_2025_26.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15faa5e",
   "metadata": {},
   "source": [
    "El CSV usa `;` como separador. Las columnas son:\n",
    "\n",
    "| Ãndice | Campo    | DescripciÃ³n                          | Tipo/TamaÃ±o        |\n",
    "|--------|----------|--------------------------------------|---------------------|\n",
    "| 0      | `prbarc` | CÃ³digo de barras del ejemplar        | NumÃ©rico 10         |\n",
    "| 1      | `prprsu` | CÃ³digo de sucursal del prÃ©stamo      | AlfanumÃ©rico 6      |\n",
    "| 2      | `prcolp` | CÃ³digo de tipo de lector             | AlfanumÃ©rico 3      |\n",
    "| 3      | `prcocp` | CÃ³digo de tipo de ejemplar           | AlfanumÃ©rico 3      |\n",
    "| 4      | `prlebi` | Biblioteca del lector                | AlfanumÃ©rico 6      |\n",
    "| 5      | `prlesu` | Sucursal del lector                  | AlfanumÃ©rico 6      |\n",
    "| 6      | `pradul` | Adulto (1) / Infantil (0)            | NumÃ©rico 1          |\n",
    "| 7      | `prfpre` | Fecha del prÃ©stamo (YYYY-MM-DD HH:MM:SS) | Fecha/Hora 24  |\n",
    "| 8      | `prcocs` | Soporte del ejemplar                 | AlfanumÃ©rico 3      |\n",
    "| 9      | `tititu` | TÃ­tulo (truncado a 60 caracteres)    | AlfanumÃ©rico 60     |\n",
    "| 10     | `tiauto` | Autor                                | AlfanumÃ©rico        |\n",
    "\n",
    "**CÃ³digos de soporte (`prcocs`):**\n",
    "\n",
    "| CÃ³digo | DescripciÃ³n                    | CÃ³digo | DescripciÃ³n                    |\n",
    "|--------|--------------------------------|--------|--------------------------------|\n",
    "| `LIB`  | Libro                          | `DVV`  | VÃ­deo (DVD)                    |\n",
    "| `FOL`  | Folleto                        | `DVR`  | Recurso electrÃ³nico (DVD)      |\n",
    "| `CDA`  | Audio (CD)                     | `VBR`  | VÃ­deo (Blue-Ray)               |\n",
    "| `CDR`  | Recurso electrÃ³nico (CD)       | `VHV`  | VÃ­deo (VHS)                    |\n",
    "| `CAA`  | Audio (casete)                 | `PPE`  | PublicaciÃ³n periÃ³dica          |\n",
    "| `MAP`  | Material cartogrÃ¡fico          | `MFD`  | Microformas                    |\n",
    "| `MGN`  | Grabados y carteles            | `MUI`  | MÃºsica impresa                 |\n",
    "| `DIR`  | Recurso electrÃ³nico (disquete) | `DIA`  | Diapositivas                   |\n",
    "| `OTA`  | Audio (otros)                  | `OTR`  | Recurso electrÃ³nico (otros)    |\n",
    "| `OTS`  | Otro soporte                   | `OTV`  | VÃ­deo (otros)                  |\n",
    "| `VIA`  | Audio (vinilo)                 |        |                                |\n",
    "\n",
    "## 2. MapReduce local: Mapper y Reducer en Python\n",
    "\n",
    "Antes de lanzar un trabajo en el clÃºster Hadoop, es buena prÃ¡ctica probarlo en local. Usamos el patrÃ³n **Hadoop Streaming**: los scripts leen de `stdin` y escriben a `stdout`.\n",
    "\n",
    "### Mapper\n",
    "Lee cada lÃ­nea del CSV, filtra lÃ­neas vacÃ­as y cabeceras, extrae el tÃ­tulo y emite `tÃ­tulo\\t1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b17126f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "# CSV con delimitador ';'\n",
    "# Columnas: prbarc;prprsu;prcolp;prcocp;prlebi;prlesu;pradul;prfpre;prcocs;tititu;tiauto\n",
    "reader = csv.reader(sys.stdin, delimiter=';')\n",
    "\n",
    "for row in reader:\n",
    "    # Saltar lÃ­neas vacÃ­as o la fila de cabecera\n",
    "    if not row or 'prbarc' in row[0] or 'prprsu' in row[0]:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # El Ã­ndice 9 corresponde a tititu (tÃ­tulo del libro)\n",
    "        title = row[9].strip().replace('/', '')\n",
    "\n",
    "        if title:\n",
    "            # Emitir tÃ­tulo y cuenta de 1, separados por tabulador\n",
    "            print(f\"{title}\\t1\")\n",
    "\n",
    "    except (IndexError, ValueError):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31a7da0",
   "metadata": {},
   "source": [
    "### Reducer\n",
    "Recibe las lÃ­neas ordenadas por clave (tÃ­tulo) y suma las ocurrencias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02a5bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "\n",
    "current_title = None\n",
    "current_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "\n",
    "    try:\n",
    "        title, count_str = line.rsplit('\\t', 1)\n",
    "        count = int(count_str)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    if current_title == title:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_title is not None:\n",
    "            print(f\"{current_count}\\t{current_title}\")\n",
    "\n",
    "        current_title = title\n",
    "        current_count = count\n",
    "\n",
    "if current_title is not None:\n",
    "    print(f\"{current_count}\\t{current_title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532dd18c",
   "metadata": {},
   "source": [
    "### Prueba local\n",
    "Probamos la cadena completa en local (sin Hadoop) usando pipes de Unix â€” exactamente como lo harÃ¡ Hadoop Streaming internamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b59588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Damos permisos de ejecuciÃ³n a los scripts\n",
    "!chmod +x mapper.py\n",
    "!chmod +x reducer.py\n",
    "\n",
    "# Probamos localmente con las primeras 50 lÃ­neas del CSV\n",
    "!head -n 50 data/prestamos_bibliotecas_madrid_2025_26.csv | ./mapper.py | sort | ./reducer.py | sort -rn | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23219561",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Crear directorios y subir ficheros a HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f49fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear directorio en HDFS para nuestros datos de bibliotecas\n",
    "!hadoop fs -mkdir -p libros/input\n",
    "\n",
    "# Subir el CSV de prÃ©stamos a HDFS (-f para sobreescribir si ya existe)\n",
    "!hadoop fs -put -f data/prestamos_bibliotecas_madrid_2025_26.csv libros/input\n",
    "\n",
    "# Verificar que el fichero estÃ¡ en HDFS\n",
    "!hadoop fs -ls -h libros/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781ac88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TamaÃ±o legible del fichero (-h = human-readable)\n",
    "!hadoop fs -du -h libros/input/\n",
    "    \n",
    "# Espacio en HDFS (human-readable con -h)\n",
    "!hadoop fs -df -h /"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557cdb08",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Inspeccionar ficheros en HDFS\n",
    "\n",
    "Una vez subido el fichero, podemos inspeccionarlo directamente en HDFS sin descargarlo â€” muy Ãºtil para ficheros enormes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d709a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver las primeras lÃ­neas del fichero en HDFS (cabecera + primeros registros)\n",
    "!hadoop fs -head libros/input/prestamos_bibliotecas_madrid_2025_26.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389142df",
   "metadata": {},
   "source": [
    "### InformaciÃ³n de bloques y replicaciÃ³n\n",
    "\n",
    "En HDFS, los ficheros se dividen en **bloques** (por defecto 128 MB) que se replican en varios nodos. Podemos ver esta informaciÃ³n con `fsck`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver informaciÃ³n de bloques, replicaciÃ³n y distribuciÃ³n del fichero\n",
    "!hdfs fsck libros/input/prestamos_bibliotecas_madrid_2025_26.csv -files -blocks -locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cd5f8a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Cambiar la replicaciÃ³n de un fichero\n",
    "\n",
    "Por defecto HDFS replica cada bloque 4 veces (en nuestro clÃºster con 4 workers). Podemos cambiar el factor de replicaciÃ³n con `setrep`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96253b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver la replicaciÃ³n actual (la 2Âª columna en ls muestra el factor de replicaciÃ³n)\n",
    "!hadoop fs -ls libros/input\n",
    "\n",
    "# Cambiar la replicaciÃ³n a 2\n",
    "!hadoop fs -setrep 2 libros/input/prestamos_bibliotecas_madrid_2025_26.csv\n",
    "\n",
    "# Verificar el cambio\n",
    "!hadoop fs -ls libros/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb1acd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restaurar la replicaciÃ³n a 4\n",
    "!hadoop fs -setrep 4 /user/jovyan/libros/input/prestamos_bibliotecas_madrid_2025_26.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2982595",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. MapReduce con Hadoop Streaming\n",
    "\n",
    "Ahora vamos a ejecutar nuestro mapper y reducer **en el clÃºster Hadoop** usando **Hadoop Streaming**. En lugar de procesar los datos en una sola mÃ¡quina, Hadoop distribuirÃ¡ el trabajo entre los 4 workers.\n",
    "\n",
    "### Â¿CÃ³mo funciona?\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  HDFS    â”‚â”€â”€â”€â”€â–¶â”‚   Mapper     â”‚â”€â”€â”€â”€â–¶â”‚   Sort &  â”‚â”€â”€â”€â”€â–¶â”‚   Reducer    â”‚â”€â”€â”€â”€â–¶â”‚  HDFS    â”‚\n",
    "â”‚  Input   â”‚     â”‚  (mapper.py) â”‚     â”‚  Shuffle  â”‚     â”‚ (reducer.py) â”‚     â”‚  Output  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "- **Input:** el fichero CSV en HDFS se divide en _splits_ (uno por bloque)\n",
    "- **Map:** cada worker ejecuta `mapper.py` sobre su _split_\n",
    "- **Shuffle & Sort:** Hadoop agrupa y ordena las claves intermedias (tÃ­tulos)\n",
    "- **Reduce:** `reducer.py` suma las ocurrencias de cada tÃ­tulo\n",
    "- **Output:** el resultado se escribe en HDFS\n",
    "\n",
    "### Ejecutar el trabajo MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbad7cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero eliminamos el directorio de salida si existe (Hadoop no permite sobreescribirlo)\n",
    "!hadoop fs -rm -r -f libros/output\n",
    "\n",
    "# Ejecutar MapReduce con Hadoop Streaming\n",
    "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \\\n",
    "    -files mapper.py,reducer.py \\\n",
    "    -mapper \"python3 mapper.py\" \\\n",
    "    -reducer \"python3 reducer.py\" \\\n",
    "    -input libros/input/prestamos_bibliotecas_madrid_2025_26.csv \\\n",
    "    -output libros/output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab47dae6",
   "metadata": {},
   "source": [
    "### Examinar la salida del MapReduce\n",
    "\n",
    "La salida se guarda en un directorio de HDFS. Hadoop crea un fichero `part-XXXXX` por cada reducer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a00d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver los ficheros de salida generados por MapReduce\n",
    "!hadoop fs -ls libros/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4052a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 libros mÃ¡s prestados (ordenar numÃ©ricamente en orden descendente)\n",
    "!hadoop fs -cat libros/output/part-00000 | sort -rn 2>/dev/null | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efedef4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Descargar resultados y visualizar con Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48d24c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar la salida de MapReduce a local\n",
    "!hadoop fs -get -f /user/jovyan/libros/output/part-00000 data/resultado_prestamos.tsv\n",
    "\n",
    "# Verificar\n",
    "!head -5 data/resultado_prestamos.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25dc94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Leer la salida del MapReduce (formato: count\\ttitle)\n",
    "df = pd.read_csv(\n",
    "    'data/resultado_prestamos.tsv',\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=['prestamos', 'titulo'],\n",
    "    encoding='utf-8',\n",
    "    encoding_errors='replace'\n",
    ")\n",
    "\n",
    "print(f\"Total de tÃ­tulos Ãºnicos: {len(df):,}\")\n",
    "print(f\"Total de prÃ©stamos: {df['prestamos'].sum():,}\")\n",
    "print()\n",
    "\n",
    "top = df.nlargest(50, 'prestamos')\n",
    "print(\"ðŸ“š Top 50 libros mÃ¡s prestados en las bibliotecas de Madrid:\")\n",
    "print(top.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
